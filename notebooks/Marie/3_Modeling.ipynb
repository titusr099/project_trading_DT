{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff66f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos procesados\n",
    "df = pd.read_csv(\"../../data/processed/SPLV_clean.csv\", parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8114a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y prueba por fecha\n",
    "train_df = df[df['Date'] <= '2025-02-28']\n",
    "test_df = df[df['Date'] >= '2025-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c79e4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features y target\n",
    "X_train = train_df.drop(columns=['target', 'Close', 'Open', 'High', 'Low', 'Date'])\n",
    "y_train = train_df['target']\n",
    "X_test = test_df.drop(columns=['target', 'Close', 'Open', 'High', 'Low', 'Date'])\n",
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48ed61f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight para XGBoost: 0.8726937269372693\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calcular ratio de clases\n",
    "class_counts = Counter(y_train)\n",
    "scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "print(\"scale_pos_weight para XGBoost:\", scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1c85668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta si no existe\n",
    "os.makedirs(\"../../models\", exist_ok=True)\n",
    "\n",
    "# Modelos base\n",
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        max_depth=5,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        max_depth=10,\n",
    "        n_estimators=100,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=100\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    learning_rate=0.03,            # Más bajo para evitar sobreajuste\n",
    "    n_estimators=200,              # Más árboles = mejor generalización\n",
    "    num_leaves=15,                 # Reduce complejidad\n",
    "    min_child_samples=10,          # Previene overfitting\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    importance_type='gain'\n",
    "),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=150,\n",
    "        scale_pos_weight=scale_pos_weight * 0.5,  # ⚠️ suaviza penalización\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='aucpr',  # mejor para clases desbalanceadas\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8\n",
    ")\n",
    "}\n",
    "\n",
    "# Función para entrenar y evaluar\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, threshold=0.55):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicción con threshold si existe predict_proba\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_proba > threshold).astype(int)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluaciones\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro')\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"\\n===== {model_name} (Threshold: {threshold}) =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision (macro): {prec:.4f}\")\n",
    "    print(f\"Recall (macro): {rec:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Mostrar importancia de variables si el modelo la tiene\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"Feature Importances:\")\n",
    "        for name, val in zip(X_train.columns, model.feature_importances_):\n",
    "            print(f\"{name}: {val:.4f}\")\n",
    "\n",
    "    # Guardar modelo\n",
    "    joblib.dump(model, f\"../../models/SPLV_{model_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2850d199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 676, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 316\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5648\n",
      "[LightGBM] [Info] Number of data points in the train set: 677, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 542, number of negative: 473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6405\n",
      "[LightGBM] [Info] Number of data points in the train set: 1015, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'n_estimators': [150, 200],\n",
    "    'num_leaves': [5, 10],\n",
    "    'min_child_samples': [15, 20],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=lgb.LGBMClassifier(random_state=42, class_weight='balanced'),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "models[\"LightGBM\"] = best_model  # Reemplaza el modelo por el mejor encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32ee443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DecisionTree (Threshold: 0.55) =====\n",
      "Accuracy: 0.6429\n",
      "Precision (macro): 0.6257\n",
      "Recall (macro): 0.6257\n",
      "F1 Score (macro): 0.6257\n",
      "Confusion Matrix:\n",
      "[[12  5]\n",
      " [ 5  6]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71        17\n",
      "           1       0.55      0.55      0.55        11\n",
      "\n",
      "    accuracy                           0.64        28\n",
      "   macro avg       0.63      0.63      0.63        28\n",
      "weighted avg       0.64      0.64      0.64        28\n",
      "\n",
      "Feature Importances:\n",
      "day_of_week: 0.0429\n",
      "month: 0.0000\n",
      "is_month_end: 0.0000\n",
      "price_diff: 0.0000\n",
      "pct_diff: 0.0000\n",
      "return_daily: 0.0557\n",
      "return_lag_1: 0.0000\n",
      "return_lag_2: 0.0000\n",
      "return_lag_3: 0.0215\n",
      "return_lag_4: 0.0000\n",
      "return_lag_5: 0.0517\n",
      "sma_5: 0.1026\n",
      "sma_10: 0.0000\n",
      "rolling_std_return_5: 0.0629\n",
      "RSI_5: 0.0985\n",
      "MACD: 0.0000\n",
      "MACD_signal: 0.0000\n",
      "bb_middle: 0.0000\n",
      "bb_upper: 0.0000\n",
      "bb_lower: 0.0000\n",
      "volume_outlier: 0.0000\n",
      "price_above_SMA50: 0.0000\n",
      "RSI_overbought: 0.0000\n",
      "MACD_above_signal: 0.0000\n",
      "return_3d: 0.0212\n",
      "vol_month: 0.0417\n",
      "price_vs_sma10: 0.1379\n",
      "momentum_3d: 0.0192\n",
      "vol_change: 0.0663\n",
      "sma_cross_up: 0.0000\n",
      "volatility_ratio: 0.0000\n",
      "gap_up: 0.1897\n",
      "lower_shadow: 0.0882\n",
      "\n",
      "===== RandomForest (Threshold: 0.55) =====\n",
      "Accuracy: 0.6786\n",
      "Precision (macro): 0.6742\n",
      "Recall (macro): 0.6230\n",
      "F1 Score (macro): 0.6199\n",
      "Confusion Matrix:\n",
      "[[15  2]\n",
      " [ 7  4]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.88      0.77        17\n",
      "           1       0.67      0.36      0.47        11\n",
      "\n",
      "    accuracy                           0.68        28\n",
      "   macro avg       0.67      0.62      0.62        28\n",
      "weighted avg       0.68      0.68      0.65        28\n",
      "\n",
      "Feature Importances:\n",
      "day_of_week: 0.0132\n",
      "month: 0.0178\n",
      "is_month_end: 0.0013\n",
      "price_diff: 0.0363\n",
      "pct_diff: 0.0296\n",
      "return_daily: 0.0388\n",
      "return_lag_1: 0.0369\n",
      "return_lag_2: 0.0369\n",
      "return_lag_3: 0.0449\n",
      "return_lag_4: 0.0383\n",
      "return_lag_5: 0.0401\n",
      "sma_5: 0.0314\n",
      "sma_10: 0.0326\n",
      "rolling_std_return_5: 0.0412\n",
      "RSI_5: 0.0428\n",
      "MACD: 0.0373\n",
      "MACD_signal: 0.0366\n",
      "bb_middle: 0.0310\n",
      "bb_upper: 0.0355\n",
      "bb_lower: 0.0352\n",
      "volume_outlier: 0.0028\n",
      "price_above_SMA50: 0.0047\n",
      "RSI_overbought: 0.0020\n",
      "MACD_above_signal: 0.0028\n",
      "return_3d: 0.0442\n",
      "vol_month: 0.0372\n",
      "price_vs_sma10: 0.0407\n",
      "momentum_3d: 0.0375\n",
      "vol_change: 0.0391\n",
      "sma_cross_up: 0.0018\n",
      "volatility_ratio: 0.0335\n",
      "gap_up: 0.0467\n",
      "lower_shadow: 0.0494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== AdaBoost (Threshold: 0.55) =====\n",
      "Accuracy: 0.6071\n",
      "Precision (macro): 0.3036\n",
      "Recall (macro): 0.5000\n",
      "F1 Score (macro): 0.3778\n",
      "Confusion Matrix:\n",
      "[[17  0]\n",
      " [11  0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76        17\n",
      "           1       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.61        28\n",
      "   macro avg       0.30      0.50      0.38        28\n",
      "weighted avg       0.37      0.61      0.46        28\n",
      "\n",
      "Feature Importances:\n",
      "day_of_week: 0.0000\n",
      "month: 0.0000\n",
      "is_month_end: 0.0000\n",
      "price_diff: 0.0000\n",
      "pct_diff: 0.0000\n",
      "return_daily: 0.1083\n",
      "return_lag_1: 0.0000\n",
      "return_lag_2: 0.0000\n",
      "return_lag_3: 0.0000\n",
      "return_lag_4: 0.0000\n",
      "return_lag_5: 0.0000\n",
      "sma_5: 0.0000\n",
      "sma_10: 0.3515\n",
      "rolling_std_return_5: 0.0364\n",
      "RSI_5: 0.0000\n",
      "MACD: 0.0900\n",
      "MACD_signal: 0.0193\n",
      "bb_middle: 0.0000\n",
      "bb_upper: 0.0000\n",
      "bb_lower: 0.0000\n",
      "volume_outlier: 0.0000\n",
      "price_above_SMA50: 0.0000\n",
      "RSI_overbought: 0.0000\n",
      "MACD_above_signal: 0.0000\n",
      "return_3d: 0.0915\n",
      "vol_month: 0.0000\n",
      "price_vs_sma10: 0.0627\n",
      "momentum_3d: 0.0000\n",
      "vol_change: 0.0151\n",
      "sma_cross_up: 0.0000\n",
      "volatility_ratio: 0.0000\n",
      "gap_up: 0.1784\n",
      "lower_shadow: 0.0468\n",
      "\n",
      "===== GradientBoosting (Threshold: 0.55) =====\n",
      "Accuracy: 0.6786\n",
      "Precision (macro): 0.8269\n",
      "Recall (macro): 0.5909\n",
      "F1 Score (macro): 0.5492\n",
      "Confusion Matrix:\n",
      "[[17  0]\n",
      " [ 9  2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79        17\n",
      "           1       1.00      0.18      0.31        11\n",
      "\n",
      "    accuracy                           0.68        28\n",
      "   macro avg       0.83      0.59      0.55        28\n",
      "weighted avg       0.79      0.68      0.60        28\n",
      "\n",
      "Feature Importances:\n",
      "day_of_week: 0.0060\n",
      "month: 0.0191\n",
      "is_month_end: 0.0000\n",
      "price_diff: 0.0047\n",
      "pct_diff: 0.0303\n",
      "return_daily: 0.0561\n",
      "return_lag_1: 0.0109\n",
      "return_lag_2: 0.0151\n",
      "return_lag_3: 0.0585\n",
      "return_lag_4: 0.0199\n",
      "return_lag_5: 0.0341\n",
      "sma_5: 0.0142\n",
      "sma_10: 0.0558\n",
      "rolling_std_return_5: 0.0505\n",
      "RSI_5: 0.0502\n",
      "MACD: 0.0663\n",
      "MACD_signal: 0.0415\n",
      "bb_middle: 0.0205\n",
      "bb_upper: 0.0359\n",
      "bb_lower: 0.0252\n",
      "volume_outlier: 0.0000\n",
      "price_above_SMA50: 0.0016\n",
      "RSI_overbought: 0.0000\n",
      "MACD_above_signal: 0.0023\n",
      "return_3d: 0.0718\n",
      "vol_month: 0.0343\n",
      "price_vs_sma10: 0.0645\n",
      "momentum_3d: 0.0147\n",
      "vol_change: 0.0353\n",
      "sma_cross_up: 0.0016\n",
      "volatility_ratio: 0.0352\n",
      "gap_up: 0.0784\n",
      "lower_shadow: 0.0455\n",
      "[LightGBM] [Info] Number of positive: 542, number of negative: 473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6405\n",
      "[LightGBM] [Info] Number of data points in the train set: 1015, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "===== LightGBM (Threshold: 0.55) =====\n",
      "Accuracy: 0.5357\n",
      "Precision (macro): 0.5357\n",
      "Recall (macro): 0.5374\n",
      "F1 Score (macro): 0.5303\n",
      "Confusion Matrix:\n",
      "[[9 8]\n",
      " [5 6]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58        17\n",
      "           1       0.43      0.55      0.48        11\n",
      "\n",
      "    accuracy                           0.54        28\n",
      "   macro avg       0.54      0.54      0.53        28\n",
      "weighted avg       0.56      0.54      0.54        28\n",
      "\n",
      "Feature Importances:\n",
      "day_of_week: 5.0000\n",
      "month: 14.0000\n",
      "is_month_end: 0.0000\n",
      "price_diff: 14.0000\n",
      "pct_diff: 26.0000\n",
      "return_daily: 14.0000\n",
      "return_lag_1: 17.0000\n",
      "return_lag_2: 6.0000\n",
      "return_lag_3: 33.0000\n",
      "return_lag_4: 15.0000\n",
      "return_lag_5: 17.0000\n",
      "sma_5: 13.0000\n",
      "sma_10: 17.0000\n",
      "rolling_std_return_5: 32.0000\n",
      "RSI_5: 34.0000\n",
      "MACD: 20.0000\n",
      "MACD_signal: 29.0000\n",
      "bb_middle: 0.0000\n",
      "bb_upper: 12.0000\n",
      "bb_lower: 14.0000\n",
      "volume_outlier: 0.0000\n",
      "price_above_SMA50: 2.0000\n",
      "RSI_overbought: 0.0000\n",
      "MACD_above_signal: 0.0000\n",
      "return_3d: 46.0000\n",
      "vol_month: 25.0000\n",
      "price_vs_sma10: 40.0000\n",
      "momentum_3d: 11.0000\n",
      "vol_change: 26.0000\n",
      "sma_cross_up: 1.0000\n",
      "volatility_ratio: 13.0000\n",
      "gap_up: 64.0000\n",
      "lower_shadow: 40.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [17:12:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1742444238318/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost (Threshold: 0.55) =====\n",
      "Accuracy: 0.6786\n",
      "Precision (macro): 0.7083\n",
      "Recall (macro): 0.6070\n",
      "F1 Score (macro): 0.5902\n",
      "Confusion Matrix:\n",
      "[[16  1]\n",
      " [ 8  3]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.94      0.78        17\n",
      "           1       0.75      0.27      0.40        11\n",
      "\n",
      "    accuracy                           0.68        28\n",
      "   macro avg       0.71      0.61      0.59        28\n",
      "weighted avg       0.70      0.68      0.63        28\n",
      "\n",
      "Feature Importances:\n",
      "day_of_week: 0.0263\n",
      "month: 0.0271\n",
      "is_month_end: 0.0445\n",
      "price_diff: 0.0281\n",
      "pct_diff: 0.0292\n",
      "return_daily: 0.0278\n",
      "return_lag_1: 0.0300\n",
      "return_lag_2: 0.0300\n",
      "return_lag_3: 0.0298\n",
      "return_lag_4: 0.0270\n",
      "return_lag_5: 0.0316\n",
      "sma_5: 0.0237\n",
      "sma_10: 0.0271\n",
      "rolling_std_return_5: 0.0319\n",
      "RSI_5: 0.0321\n",
      "MACD: 0.0327\n",
      "MACD_signal: 0.0273\n",
      "bb_middle: 0.0300\n",
      "bb_upper: 0.0297\n",
      "bb_lower: 0.0320\n",
      "volume_outlier: 0.0166\n",
      "price_above_SMA50: 0.0321\n",
      "RSI_overbought: 0.0506\n",
      "MACD_above_signal: 0.0194\n",
      "return_3d: 0.0317\n",
      "vol_month: 0.0323\n",
      "price_vs_sma10: 0.0278\n",
      "momentum_3d: 0.0278\n",
      "vol_change: 0.0258\n",
      "sma_cross_up: 0.0479\n",
      "volatility_ratio: 0.0266\n",
      "gap_up: 0.0310\n",
      "lower_shadow: 0.0326\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento de todos los modelos\n",
    "for model_name, model in models.items():\n",
    "    train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, threshold=0.55)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
