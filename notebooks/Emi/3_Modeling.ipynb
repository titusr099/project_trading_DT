{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones y librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix,  mean_squared_error, classification_report\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV,  train_test_split, GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv('../../data/processed/JNJ_clean.csv')\n",
    "\n",
    "# Crear las variables de entrenamiento y prueba, asegurándose de que los datos de entrenamiento sean hasta febrero de 2025\n",
    "train_df = df[df['Date'] <= '2025-02-28']\n",
    "test_df = df[df['Date'] >= '2025-03-01']\n",
    "\n",
    "# Características (X) y objetivo (y)\n",
    "X_train = train_df.drop(columns=['target', 'Close', 'Open', 'High','Low', 'Date'])\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_test = test_df.drop(columns=['target', 'Close', 'Open', 'High','Low','Date'])\n",
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento y evaluación\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name, is_classification=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if is_classification:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        print(f'{model_name} Accuracy: {accuracy:.4f}')\n",
    "        print(f'{model_name} Precision (Macro): {precision:.4f}')\n",
    "        print(f'{model_name} Recall (Macro): {recall:.4f}')\n",
    "        print(f'Matriz de Confusión para {model_name}:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "        print(f'Reporte de Clasificación para {model_name}:\\n{classification_report(y_test, y_pred)}')\n",
    "        \n",
    "        # Importancia de características (si está disponible)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importances = model.feature_importances_\n",
    "            # Se asume que X_train es un DataFrame para tener nombres de columnas\n",
    "            feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "            print(f'Importancia de Características para {model_name}:')\n",
    "            for feature, importance in zip(feature_names, feature_importances):\n",
    "                print(f'{feature}: {importance:.4f}')\n",
    "        else:\n",
    "            print(f'{model_name} no tiene importancias disponibles.')\n",
    "    else:\n",
    "        # Ejemplo para regresión (en caso de ser necesario)\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f'{model_name} RMSE: {rmse:.4f}')\n",
    "        \n",
    "    # Guardar el modelo entrenado\n",
    "    joblib.dump(model, f'../../models/{model_name}_JNJ.pkl')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de búsqueda de hiperparámetros y validación cruzada\n",
    "def hyperparameter_tuning(model, X_train, y_train, param_grid, is_classification=True):\n",
    "    # Se define la métrica a 'precision_macro' si es clasificación\n",
    "    scoring = 'precision_macro' if is_classification else 'neg_mean_squared_error'\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = RandomizedSearchCV(model, \n",
    "                                     param_distributions=param_grid, \n",
    "                                     n_iter=10,  # Se aumentó para explorar más combinaciones\n",
    "                                     cv=cv, \n",
    "                                     random_state=42, \n",
    "                                     n_jobs=-1, \n",
    "                                     scoring=\"precision\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Mejores parámetros para {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de modelos base\n",
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(),random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "# Rangos de hiperparámetros ampliados\n",
    "param_grids = {\n",
    "    \"DecisionTree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],      # Tipos de función de impureza\n",
    "        \"max_depth\": [3, 5, 10, 20, 30, None],              # Límite de profundidad (más opciones)\n",
    "        \"min_samples_split\": [2, 5, 10, 20],                # Mínimo para dividir un nodo\n",
    "        \"min_samples_leaf\": [1, 2, 5, 10],                  # Mínimo de muestras en hojas\n",
    "        \"max_features\": [None, \"sqrt\", \"log2\"],             # Subconjunto de variables para división\n",
    "        \"class_weight\": [None, \"balanced\"] \n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [200, 500, 800],\n",
    "        \"max_depth\": [10, 20, 30, None],\n",
    "        \"max_features\": ['sqrt', 'log2', None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"n_estimators\": [100, 200, 300, 400],        # Más iteraciones (mejor capacidad)\n",
    "        \"learning_rate\": [0.01,0.05, 0.1, 0.3, 0.5, 1.0],  # Fino control del aprendizaje\n",
    "        \"estimator__max_depth\": [1, 2, 3],           # Control de complejidad del árbol\n",
    "        \"estimator__min_samples_split\": [2, 5, 10],  # Regularización\n",
    "        \"estimator__min_samples_leaf\": [1, 2, 5],    # Más regularización\n",
    "        \"estimator__class_weight\": [None, \"balanced\"]  # Mejor manejo del desbalance\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"n_estimators\": [100, 200, 300],         # Más árboles para mayor capacidad\n",
    "        \"learning_rate\": [0.001, 0.05, 0.1, 0.2],       # Aprendizaje más rápido\n",
    "        \"max_depth\": [4, 6, 8],                  # Mayor profundidad para captar interacciones\n",
    "        \"min_samples_split\": [2, 5, 10],         # Regularización ligera (evita sobreajuste)\n",
    "        \"min_samples_leaf\": [1, 3, 5],           # Controla hojas pequeñas que podrían sobreajustar\n",
    "        \"subsample\": [0.8, 1.0],                 # Stochastic gradient boosting (para reducir overfitting)\n",
    "        \"max_features\": ['sqrt', 'log2', None] \n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"n_estimators\": [50, 100, 200, 500],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.05, 0.1],\n",
    "        \"num_leaves\": [31, 50, 70, 100],\n",
    "        \"min_child_samples\": [5, 10, 20],\n",
    "        \"max_depth\": [-1, 3, 5, 7, 10],\n",
    "        \"feature_fraction\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"bagging_fraction\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"bagging_freq\": [0, 1, 5],\n",
    "        \"min_gain_to_split\": [0, 0.01, 0.1],\n",
    "        \"lambda_l1\": [0, 0.1, 1],\n",
    "        \"lambda_l2\": [0, 0.1, 1]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [300, 500],              # Capacidad razonable sin exagerar\n",
    "        \"learning_rate\": [0.001,0.01, 0.05, 0.1],            # Eliminamos extremos poco útiles\n",
    "        \"max_depth\": [3, 5, 7],                  # Suficiente para captar relaciones\n",
    "        \"subsample\": [0.7, 1.0],                 # Aleatoriedad en filas\n",
    "        \"colsample_bytree\": [0.7, 1.0],          # Aleatoriedad en columnas\n",
    "        \"scale_pos_weight\": [1, 5]               # Control básico de desbalance\n",
    "        },\n",
    "    \"CatBoost\": {\n",
    "        \"iterations\": [500, 1000, 1500],            # Mantener un rango equilibrado, 2000 puede ser demasiado si hay overfitting\n",
    "        \"learning_rate\": [0.001,0.01, 0.05, 0.1],          # 0.001 puede ser muy lento, lo quitamos\n",
    "        \"depth\": [4, 6, 8, 10],                      # OK\n",
    "        \"l2_leaf_reg\": [1, 3, 5, 7, 9],              # Regularización L2 para evitar sobreajuste\n",
    "        \"bagging_temperature\": [0, 0.5, 1.0],        # Controla la aleatoriedad del bootstrapping\n",
    "        \"random_strength\": [0.5, 1, 2],              # Regularización del uso de características\n",
    "        \"border_count\": [32, 64, 128],               # Cantidad de splits posibles en variables continuas\n",
    "        \"grow_policy\": ['SymmetricTree', 'Depthwise', 'Lossguide'],  # Estructura de árboles\n",
    "        \"eval_metric\": ['Accuracy', 'F1']           # Métricas de evaluación\n",
    "    }\n",
    "}\n",
    "\n",
    "# Variables para almacenar el mejor modelo global y su score\n",
    "best_model = None\n",
    "best_score = -np.inf  # Se maximiza la métrica (precisión macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando DecisionTree...\n",
      "Mejores parámetros para DecisionTreeClassifier: {'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 3, 'criterion': 'log_loss', 'class_weight': 'balanced'}\n",
      "DecisionTree Accuracy: 0.4667\n",
      "DecisionTree Precision (Macro): 0.4750\n",
      "DecisionTree Recall (Macro): 0.4777\n",
      "Matriz de Confusión para DecisionTree:\n",
      "[[ 9  5]\n",
      " [11  5]]\n",
      "Reporte de Clasificación para DecisionTree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.64      0.53        14\n",
      "           1       0.50      0.31      0.38        16\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.47      0.48      0.46        30\n",
      "weighted avg       0.48      0.47      0.45        30\n",
      "\n",
      "Importancia de Características para DecisionTree:\n",
      "log_vol: 0.0000\n",
      "year: 0.0000\n",
      "month: 0.0000\n",
      "day: 0.0000\n",
      "day_of_week: 0.0000\n",
      "is_month_end: 0.0000\n",
      "price_diff: 0.0000\n",
      "pct_diff: 0.0000\n",
      "return_daily: 0.0000\n",
      "return_lag_1: 0.0000\n",
      "return_lag_2: 0.0000\n",
      "return_lag_3: 0.1160\n",
      "return_lag_4: 0.2295\n",
      "return_lag_5: 0.0000\n",
      "sma_5: 0.1175\n",
      "rolling_std_return_5: 0.0000\n",
      "RSI_5: 0.0000\n",
      "MACD: 0.0569\n",
      "MACD_signal: 0.0000\n",
      "bb_middle: 0.2679\n",
      "bb_upper: 0.0000\n",
      "bb_lower: 0.1137\n",
      "rolling_mean_vol: 0.0000\n",
      "rolling_std_vol: 0.0983\n",
      "volume_spike: 0.0000\n",
      "price_above_SMA50: 0.0000\n",
      "RSI_overbought: 0.0000\n",
      "MACD_above_signal: 0.0000\n",
      "Precisión (Macro) en Test para DecisionTree: 0.4750\n"
     ]
    }
   ],
   "source": [
    "model_name = \"DecisionTree\"\n",
    "model = models[model_name]\n",
    "print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "# Ajuste de hiperparámetros usando validación cruzada para DecisionTree\n",
    "model_tuned = hyperparameter_tuning(model, X_train, y_train, param_grids[model_name], is_classification=True)\n",
    "\n",
    "# Entrenamiento y evaluación del modelo\n",
    "trained_model = train_and_evaluate(model_tuned, X_train, X_test, y_train, y_test, model_name, is_classification=True)\n",
    "\n",
    "# Evaluación basada en la precisión macro\n",
    "score = precision_score(y_test, trained_model.predict(X_test), average='macro')\n",
    "print(f\"Precisión (Macro) en Test para {model_name}: {score:.4f}\")\n",
    "\n",
    "# Actualización del mejor modelo en función de la métrica\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_model = trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando RandomForest...\n",
      "Mejores parámetros para RandomForestClassifier: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 20, 'class_weight': None, 'bootstrap': True}\n",
      "RandomForest Accuracy: 0.5333\n",
      "RandomForest Precision (Macro): 0.5312\n",
      "RandomForest Recall (Macro): 0.5312\n",
      "Matriz de Confusión para RandomForest:\n",
      "[[7 7]\n",
      " [7 9]]\n",
      "Reporte de Clasificación para RandomForest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50        14\n",
      "           1       0.56      0.56      0.56        16\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.53      0.53      0.53        30\n",
      "weighted avg       0.53      0.53      0.53        30\n",
      "\n",
      "Importancia de Características para RandomForest:\n",
      "log_vol: 0.0493\n",
      "year: 0.0132\n",
      "month: 0.0275\n",
      "day: 0.0376\n",
      "day_of_week: 0.0204\n",
      "is_month_end: 0.0006\n",
      "price_diff: 0.0396\n",
      "pct_diff: 0.0405\n",
      "return_daily: 0.0446\n",
      "return_lag_1: 0.0438\n",
      "return_lag_2: 0.0460\n",
      "return_lag_3: 0.0508\n",
      "return_lag_4: 0.0570\n",
      "return_lag_5: 0.0445\n",
      "sma_5: 0.0453\n",
      "rolling_std_return_5: 0.0505\n",
      "RSI_5: 0.0496\n",
      "MACD: 0.0468\n",
      "MACD_signal: 0.0495\n",
      "bb_middle: 0.0431\n",
      "bb_upper: 0.0430\n",
      "bb_lower: 0.0464\n",
      "rolling_mean_vol: 0.0457\n",
      "rolling_std_vol: 0.0482\n",
      "volume_spike: 0.0032\n",
      "price_above_SMA50: 0.0038\n",
      "RSI_overbought: 0.0041\n",
      "MACD_above_signal: 0.0054\n",
      "Precisión (Macro) en Test para RandomForest: 0.5312\n"
     ]
    }
   ],
   "source": [
    "model_name = \"RandomForest\"\n",
    "model = models[model_name]\n",
    "print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "# Ajuste de hiperparámetros usando validación cruzada para RandomForest\n",
    "model_tuned = hyperparameter_tuning(model, X_train, y_train, param_grids[model_name], is_classification=True)\n",
    "\n",
    "# Entrenamiento y evaluación del modelo\n",
    "trained_model = train_and_evaluate(model_tuned, X_train, X_test, y_train, y_test, model_name, is_classification=True)\n",
    "\n",
    "# Evaluación basada en la precisión macro\n",
    "score = precision_score(y_test, trained_model.predict(X_test), average='macro')\n",
    "print(f\"Precisión (Macro) en Test para {model_name}: {score:.4f}\")\n",
    "\n",
    "# Actualización del mejor modelo en función de la métrica\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_model = trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando AdaBoost...\n",
      "Mejores parámetros para AdaBoostClassifier: {'n_estimators': 100, 'learning_rate': 1.0, 'estimator__min_samples_split': 10, 'estimator__min_samples_leaf': 5, 'estimator__max_depth': 1, 'estimator__class_weight': 'balanced'}\n",
      "AdaBoost Accuracy: 0.4333\n",
      "AdaBoost Precision (Macro): 0.4367\n",
      "AdaBoost Recall (Macro): 0.4375\n",
      "Matriz de Confusión para AdaBoost:\n",
      "[[ 7  7]\n",
      " [10  6]]\n",
      "Reporte de Clasificación para AdaBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.50      0.45        14\n",
      "           1       0.46      0.38      0.41        16\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.44      0.44      0.43        30\n",
      "weighted avg       0.44      0.43      0.43        30\n",
      "\n",
      "Importancia de Características para AdaBoost:\n",
      "log_vol: 0.0000\n",
      "year: 0.0000\n",
      "month: 0.0000\n",
      "day: 0.0685\n",
      "day_of_week: 0.0000\n",
      "is_month_end: 0.0000\n",
      "price_diff: 0.0000\n",
      "pct_diff: 0.0000\n",
      "return_daily: 0.0000\n",
      "return_lag_1: 0.0000\n",
      "return_lag_2: 0.0000\n",
      "return_lag_3: 0.0439\n",
      "return_lag_4: 0.1556\n",
      "return_lag_5: 0.0000\n",
      "sma_5: 0.0856\n",
      "rolling_std_return_5: 0.0342\n",
      "RSI_5: 0.0642\n",
      "MACD: 0.0485\n",
      "MACD_signal: 0.1499\n",
      "bb_middle: 0.0797\n",
      "bb_upper: 0.0000\n",
      "bb_lower: 0.1015\n",
      "rolling_mean_vol: 0.0603\n",
      "rolling_std_vol: 0.1081\n",
      "volume_spike: 0.0000\n",
      "price_above_SMA50: 0.0000\n",
      "RSI_overbought: 0.0000\n",
      "MACD_above_signal: 0.0000\n",
      "Precisión (Macro) en Test para AdaBoost: 0.4367\n"
     ]
    }
   ],
   "source": [
    "model_name = \"AdaBoost\"\n",
    "model = models[model_name]\n",
    "print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "# Ajuste de hiperparámetros usando validación cruzada para AdaBoost\n",
    "model_tuned = hyperparameter_tuning(model, X_train, y_train, param_grids[model_name], is_classification=True)\n",
    "\n",
    "# Entrenamiento y evaluación del modelo\n",
    "trained_model = train_and_evaluate(model_tuned, X_train, X_test, y_train, y_test, model_name, is_classification=True)\n",
    "\n",
    "# Evaluación basada en la precisión macro\n",
    "score = precision_score(y_test, trained_model.predict(X_test), average='macro')\n",
    "print(f\"Precisión (Macro) en Test para {model_name}: {score:.4f}\")\n",
    "\n",
    "# Actualización del mejor modelo en función de la métrica\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_model = trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando GradientBoosting...\n",
      "Mejores parámetros para GradientBoostingClassifier: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.2}\n",
      "GradientBoosting Accuracy: 0.5000\n",
      "GradientBoosting Precision (Macro): 0.5045\n",
      "GradientBoosting Recall (Macro): 0.5045\n",
      "Matriz de Confusión para GradientBoosting:\n",
      "[[8 6]\n",
      " [9 7]]\n",
      "Reporte de Clasificación para GradientBoosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.57      0.52        14\n",
      "           1       0.54      0.44      0.48        16\n",
      "\n",
      "    accuracy                           0.50        30\n",
      "   macro avg       0.50      0.50      0.50        30\n",
      "weighted avg       0.51      0.50      0.50        30\n",
      "\n",
      "Importancia de Características para GradientBoosting:\n",
      "log_vol: 0.0439\n",
      "year: 0.0091\n",
      "month: 0.0255\n",
      "day: 0.0335\n",
      "day_of_week: 0.0210\n",
      "is_month_end: 0.0000\n",
      "price_diff: 0.0452\n",
      "pct_diff: 0.0271\n",
      "return_daily: 0.0402\n",
      "return_lag_1: 0.0376\n",
      "return_lag_2: 0.0431\n",
      "return_lag_3: 0.0623\n",
      "return_lag_4: 0.0618\n",
      "return_lag_5: 0.0545\n",
      "sma_5: 0.0445\n",
      "rolling_std_return_5: 0.0477\n",
      "RSI_5: 0.0617\n",
      "MACD: 0.0519\n",
      "MACD_signal: 0.0581\n",
      "bb_middle: 0.0417\n",
      "bb_upper: 0.0395\n",
      "bb_lower: 0.0387\n",
      "rolling_mean_vol: 0.0454\n",
      "rolling_std_vol: 0.0531\n",
      "volume_spike: 0.0003\n",
      "price_above_SMA50: 0.0044\n",
      "RSI_overbought: 0.0021\n",
      "MACD_above_signal: 0.0062\n",
      "Precisión (Macro) en Test para GradientBoosting: 0.5045\n"
     ]
    }
   ],
   "source": [
    "model_name = \"GradientBoosting\"\n",
    "model = models[model_name]\n",
    "print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "# Ajuste de hiperparámetros usando validación cruzada para GradientBoosting\n",
    "model_tuned = hyperparameter_tuning(model, X_train, y_train, param_grids[model_name], is_classification=True)\n",
    "\n",
    "# Entrenamiento y evaluación del modelo\n",
    "trained_model = train_and_evaluate(model_tuned, X_train, X_test, y_train, y_test, model_name, is_classification=True)\n",
    "\n",
    "# Evaluación basada en la precisión macro\n",
    "score = precision_score(y_test, trained_model.predict(X_test), average='macro')\n",
    "print(f\"Precisión (Macro) en Test para {model_name}: {score:.4f}\")\n",
    "\n",
    "# Actualización del mejor modelo en función de la métrica\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_model = trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando LightGBM...\n",
      "Mejores parámetros para LGBMClassifier: {'num_leaves': 70, 'n_estimators': 200, 'min_gain_to_split': 0.01, 'min_child_samples': 20, 'max_depth': 10, 'learning_rate': 0.1, 'lambda_l2': 1, 'lambda_l1': 1, 'feature_fraction': 0.8, 'bagging_freq': 5, 'bagging_fraction': 0.7}\n",
      "LightGBM Accuracy: 0.6000\n",
      "LightGBM Precision (Macro): 0.5982\n",
      "LightGBM Recall (Macro): 0.5982\n",
      "Matriz de Confusión para LightGBM:\n",
      "[[ 8  6]\n",
      " [ 6 10]]\n",
      "Reporte de Clasificación para LightGBM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        14\n",
      "           1       0.62      0.62      0.62        16\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.60      0.60      0.60        30\n",
      "weighted avg       0.60      0.60      0.60        30\n",
      "\n",
      "Importancia de Características para LightGBM:\n",
      "log_vol: 267.0000\n",
      "year: 54.0000\n",
      "month: 101.0000\n",
      "day: 182.0000\n",
      "day_of_week: 54.0000\n",
      "is_month_end: 1.0000\n",
      "price_diff: 162.0000\n",
      "pct_diff: 120.0000\n",
      "return_daily: 194.0000\n",
      "return_lag_1: 273.0000\n",
      "return_lag_2: 285.0000\n",
      "return_lag_3: 275.0000\n",
      "return_lag_4: 272.0000\n",
      "return_lag_5: 240.0000\n",
      "sma_5: 153.0000\n",
      "rolling_std_return_5: 249.0000\n",
      "RSI_5: 281.0000\n",
      "MACD: 174.0000\n",
      "MACD_signal: 216.0000\n",
      "bb_middle: 25.0000\n",
      "bb_upper: 147.0000\n",
      "bb_lower: 150.0000\n",
      "rolling_mean_vol: 266.0000\n",
      "rolling_std_vol: 275.0000\n",
      "volume_spike: 5.0000\n",
      "price_above_SMA50: 6.0000\n",
      "RSI_overbought: 3.0000\n",
      "MACD_above_signal: 5.0000\n",
      "Precisión (Macro) en Test para LightGBM: 0.5982\n"
     ]
    }
   ],
   "source": [
    "model_name = \"LightGBM\"\n",
    "model = models[model_name]\n",
    "print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "# Ajuste de hiperparámetros usando validación cruzada para LightGBM\n",
    "model_tuned = hyperparameter_tuning(model, X_train, y_train, param_grids[model_name], is_classification=True)\n",
    "\n",
    "# Entrenamiento y evaluación del modelo\n",
    "trained_model = train_and_evaluate(model_tuned, X_train, X_test, y_train, y_test, model_name, is_classification=True)\n",
    "\n",
    "# Evaluación basada en la precisión macro\n",
    "score = precision_score(y_test, trained_model.predict(X_test), average='macro')\n",
    "print(f\"Precisión (Macro) en Test para {model_name}: {score:.4f}\")\n",
    "\n",
    "# Actualización del mejor modelo en función de la métrica\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_model = trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando XGBoost...\n",
      "Mejores parámetros para XGBClassifier: {'subsample': 0.7, 'scale_pos_weight': 1, 'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost Accuracy: 0.6000\n",
      "XGBoost Precision (Macro): 0.6027\n",
      "XGBoost Recall (Macro): 0.6027\n",
      "Matriz de Confusión para XGBoost:\n",
      "[[9 5]\n",
      " [7 9]]\n",
      "Reporte de Clasificación para XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60        14\n",
      "           1       0.64      0.56      0.60        16\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.60      0.60      0.60        30\n",
      "weighted avg       0.61      0.60      0.60        30\n",
      "\n",
      "Importancia de Características para XGBoost:\n",
      "log_vol: 0.0375\n",
      "year: 0.0314\n",
      "month: 0.0346\n",
      "day: 0.0337\n",
      "day_of_week: 0.0351\n",
      "is_month_end: 0.0428\n",
      "price_diff: 0.0332\n",
      "pct_diff: 0.0438\n",
      "return_daily: 0.0367\n",
      "return_lag_1: 0.0344\n",
      "return_lag_2: 0.0366\n",
      "return_lag_3: 0.0365\n",
      "return_lag_4: 0.0389\n",
      "return_lag_5: 0.0333\n",
      "sma_5: 0.0418\n",
      "rolling_std_return_5: 0.0388\n",
      "RSI_5: 0.0370\n",
      "MACD: 0.0386\n",
      "MACD_signal: 0.0424\n",
      "bb_middle: 0.0000\n",
      "bb_upper: 0.0414\n",
      "bb_lower: 0.0433\n",
      "rolling_mean_vol: 0.0349\n",
      "rolling_std_vol: 0.0363\n",
      "volume_spike: 0.0371\n",
      "price_above_SMA50: 0.0386\n",
      "RSI_overbought: 0.0292\n",
      "MACD_above_signal: 0.0321\n",
      "Precisión (Macro) en Test para XGBoost: 0.6027\n"
     ]
    }
   ],
   "source": [
    "model_name = \"XGBoost\"\n",
    "model = models[model_name]\n",
    "print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "# Ajuste de hiperparámetros usando validación cruzada para XGBoost\n",
    "model_tuned = hyperparameter_tuning(model, X_train, y_train, param_grids[model_name], is_classification=True)\n",
    "\n",
    "# Entrenamiento y evaluación del modelo\n",
    "trained_model = train_and_evaluate(model_tuned, X_train, X_test, y_train, y_test, model_name, is_classification=True)\n",
    "\n",
    "# Evaluación basada en la precisión macro\n",
    "score = precision_score(y_test, trained_model.predict(X_test), average='macro')\n",
    "print(f\"Precisión (Macro) en Test para {model_name}: {score:.4f}\")\n",
    "\n",
    "# Actualización del mejor modelo en función de la métrica\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_model = trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando CatBoost...\n",
      "Mejores parámetros para CatBoostClassifier: {'random_strength': 2, 'learning_rate': 0.001, 'l2_leaf_reg': 1, 'iterations': 500, 'grow_policy': 'Lossguide', 'eval_metric': 'Accuracy', 'depth': 4, 'border_count': 128, 'bagging_temperature': 0.5}\n",
      "CatBoost Accuracy: 0.6000\n",
      "CatBoost Precision (Macro): 0.6250\n",
      "CatBoost Recall (Macro): 0.5804\n",
      "Matriz de Confusión para CatBoost:\n",
      "[[ 4 10]\n",
      " [ 2 14]]\n",
      "Reporte de Clasificación para CatBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.29      0.40        14\n",
      "           1       0.58      0.88      0.70        16\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.62      0.58      0.55        30\n",
      "weighted avg       0.62      0.60      0.56        30\n",
      "\n",
      "Importancia de Características para CatBoost:\n",
      "log_vol: 3.9804\n",
      "year: 2.3454\n",
      "month: 3.3097\n",
      "day: 2.1614\n",
      "day_of_week: 3.0106\n",
      "is_month_end: 0.4375\n",
      "price_diff: 2.4432\n",
      "pct_diff: 2.0801\n",
      "return_daily: 4.1629\n",
      "return_lag_1: 3.0729\n",
      "return_lag_2: 4.1953\n",
      "return_lag_3: 2.9405\n",
      "return_lag_4: 9.2271\n",
      "return_lag_5: 1.9956\n",
      "sma_5: 7.4345\n",
      "rolling_std_return_5: 4.9613\n",
      "RSI_5: 4.7742\n",
      "MACD: 3.7898\n",
      "MACD_signal: 4.6628\n",
      "bb_middle: 6.5754\n",
      "bb_upper: 5.5782\n",
      "bb_lower: 6.0885\n",
      "rolling_mean_vol: 2.7929\n",
      "rolling_std_vol: 5.4723\n",
      "volume_spike: 0.6996\n",
      "price_above_SMA50: 0.4142\n",
      "RSI_overbought: 0.7628\n",
      "MACD_above_signal: 0.6308\n",
      "Precisión (Macro) en Test para CatBoost: 0.6250\n"
     ]
    }
   ],
   "source": [
    "model_name = \"CatBoost\"\n",
    "model = models[model_name]\n",
    "print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "# Ajuste de hiperparámetros usando validación cruzada para CatBoost\n",
    "model_tuned = hyperparameter_tuning(model, X_train, y_train, param_grids[model_name], is_classification=True)\n",
    "\n",
    "# Entrenamiento y evaluación del modelo\n",
    "trained_model = train_and_evaluate(model_tuned, X_train, X_test, y_train, y_test, model_name, is_classification=True)\n",
    "\n",
    "# Evaluación basada en la precisión macro\n",
    "score = precision_score(y_test, trained_model.predict(X_test), average='macro')\n",
    "print(f\"Precisión (Macro) en Test para {model_name}: {score:.4f}\")\n",
    "\n",
    "# Actualización del mejor modelo en función de la métrica\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_model = trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor Modelo: <catboost.core.CatBoostClassifier object at 0x317e9bbe0> con precisión (macro) 0.6250\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMejor Modelo: {best_model} con precisión (macro) {best_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolsa_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
